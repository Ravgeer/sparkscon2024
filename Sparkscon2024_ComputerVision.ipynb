{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravgeer/sparkscon2024/blob/main/Sparkscon2024_ComputerVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRL1H5IhphHt"
      },
      "source": [
        "---\n",
        "# **1. Clone Data Directory, Pretrained Models, Scripts and Augmentations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8kPMrmnYLsA",
        "outputId": "db5bc48d-f292-4ab2-dfe7-92d9ec567e72",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sparkscon2024'...\n",
            "remote: Enumerating objects: 5731, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 5731 (delta 1), reused 0 (delta 0), pack-reused 5722\u001b[K\n",
            "Receiving objects: 100% (5731/5731), 210.20 MiB | 39.46 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "Updating files: 100% (5751/5751), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ravgeer/sparkscon2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFr3j0YgpVht"
      },
      "source": [
        "---\n",
        "# **2. Clone and Install YOLO and download pre-trained models for Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "072a8022-e5df-4d02-fa41-dc9385e98441",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-326-gec331cbd Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 30.7/201.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDVxFZLdhXX3",
        "outputId": "8beeae2b-942f-4387-cd5b-ad0c72ef2896",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.87M/4.87M [00:00<00:00, 75.9MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.5M/10.5M [00:00<00:00, 167MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.9M/50.9M [00:00<00:00, 195MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.9M/24.9M [00:00<00:00, 111MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92.0M/92.0M [00:00<00:00, 327MB/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from utils.downloads import attempt_download\n",
        "p5 = ['n','s','l','m','x']\n",
        "\n",
        "cls = [f'{x}-cls' for x in p5]\n",
        "\n",
        "for x in cls:\n",
        "  attempt_download(f'weights/yolov5{x}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro_1BxWKa6UK"
      },
      "source": [
        "---\n",
        "# **3. Generate Augmentations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcb5B2rc_Uy_"
      },
      "outputs": [],
      "source": [
        "### Python Script for image scraping from Weblinks\n",
        "!python /content/yolov5/sparkscon2024/scripts/scrape_images.py 'https://www.shutterstock.com/search/granny-smith?image_type=photo' '/content/scrape_images/apples/granny_smith_new' --delay 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yOjXNpp_RaC"
      },
      "outputs": [],
      "source": [
        "### Python Script for image Augmentation from data Source\n",
        "!python /content/yolov5/sparkscon2024/scripts/augmented_images.py '/content/sparkscon2024/scrape_images/apples/train/braeburn' '/content/sparkscon2024/scrape_images/apples/train/braeburn_new' --num_augmentations 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljfxvCpKIz1W"
      },
      "source": [
        "---\n",
        "# **4. Image Classification Training**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dJ-YGB0Vjy6"
      },
      "outputs": [],
      "source": [
        "#Connect G-Drive to exchange data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Vl6gAOwYCtB",
        "outputId": "b74f7678-d252-4649-a8a1-ce08dc19a580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-326-gec331cbd Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected_device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model summary: 215 layers, 26535976 parameters, 0 gradients, 69.3 GFLOPs\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for --imgsz 640\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla T4) 14.75G total, 0.38G reserved, 0.23G allocated, 14.14G free\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
            "    26535976       68.32         0.407         13.37           nan        (1, 3, 640, 640)               (1, 1000)\n",
            "    26535976       136.6         0.354         23.89           nan        (2, 3, 640, 640)               (2, 1000)\n",
            "    26535976       273.3         0.449         35.88           nan        (4, 3, 640, 640)               (4, 1000)\n",
            "    26535976       546.5         0.646         54.59           nan        (8, 3, 640, 640)               (8, 1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 242 for CUDA:0 11.90G/14.75G (81%) âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    26535976        1093         1.040         99.62           nan       (16, 3, 640, 640)              (16, 1000)\n",
            "Optimal batch size: 242\n"
          ]
        }
      ],
      "source": [
        "### set optimal_batch_size\n",
        "import torch\n",
        "from utils.torch_utils import select_device\n",
        "from utils.autobatch import check_train_batch_size\n",
        "\n",
        "def find_optimal_batch(weights, imgsz=640, amp=True):\n",
        "    # Select the device (GPU or CPU)\n",
        "    device = select_device('')\n",
        "    print(f\"selected_device: {device}\")\n",
        "    # Load the model\n",
        "    model = torch.load(weights, map_location=device)['model'].float()  # Load FP32 model\n",
        "    model.fuse()  # Fuse Conv2d + BatchNorm2d layers\n",
        "    if amp:\n",
        "        model.half()  # Convert model to FP16 if amp (automatic mixed precision) is enabled\n",
        "\n",
        "    # Find optimal batch size\n",
        "    return check_train_batch_size(model, imgsz=imgsz, amp=amp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    weights = '/content/yolov5/weights/yolov5l-cls.pt'  # Path to your weights file\n",
        "    imgsz = 640  # Image size\n",
        "    amp = True  # Automatic Mixed Precision (AMP)\n",
        "\n",
        "    optimal_batch_size = find_optimal_batch(weights, imgsz, amp)\n",
        "    print(f\"Optimal batch size: {optimal_batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1idt5_xMc0Hz",
        "outputId": "c2b29ee1-256a-412d-8a31-3d9f50bb983f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-19 14:28:45.785250: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-19 14:28:45.785318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-19 14:28:45.889222: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=/content/sparkscon2024/scrape_images/apples/, epochs=2, batch_size=242, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=yolov5s_results, exist_ok=False, pretrained=True, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-326-gec331cbd Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 55.7MB/s]\n",
            "\n",
            "Model summary: 149 layers, 4176323 parameters, 4176323 gradients, 10.5 GFLOPs\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
            "Image sizes 224 train, 224 test\n",
            "Using 1 dataloader workers\n",
            "Logging results to \u001b[1mruns/train-cls/yolov5s_results\u001b[0m\n",
            "Starting yolov5s-cls.pt training on /content/sparkscon2024/scrape_images/apples dataset with 3 classes for 2 epochs...\n",
            "\n",
            "     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n",
            "       1/2     4.67G       0.442       0.511       0.953           1: 100% 24/24 [00:27<00:00,  1.13s/it]\n",
            "       2/2     4.67G       0.337       0.404       0.976           1: 100% 24/24 [00:22<00:00,  1.07it/s]\n",
            "\n",
            "Training complete (0.014 hours)\n",
            "Results saved to \u001b[1mruns/train-cls/yolov5s_results\u001b[0m\n",
            "Predict:         python classify/predict.py --weights runs/train-cls/yolov5s_results/weights/best.pt --source im.jpg\n",
            "Validate:        python classify/val.py --weights runs/train-cls/yolov5s_results/weights/best.pt --data /content/sparkscon2024/scrape_images/apples\n",
            "Export:          python export.py --weights runs/train-cls/yolov5s_results/weights/best.pt --include onnx\n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/yolov5s_results/weights/best.pt')\n",
            "Visualize:       https://netron.app\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## optimal_batch_size ##\n",
        "!python -W ignore classify/train.py --batch {optimal_batch_size} --epochs 2 --data  /content/sparkscon2024/scrape_images/apples/ --model yolov5s-cls.pt --name yolov5s_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz0MylfV_ORw"
      },
      "outputs": [],
      "source": [
        "### User defined batch size ##\n",
        "!python -W ignore classify/train.py --img 640 --batch 16 --epochs 2 --data  /content/sparkscon2024/scrape_images/apples/ --model yolov5l-cls.pt --name yolov5l_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sakisRB_Z24"
      },
      "outputs": [],
      "source": [
        "#### We can copy our trained models in Google drive with this command\n",
        "!cp -r /content/yolov5/runs/train-cls/  /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OonQ7MDZ8Dc"
      },
      "source": [
        "---\n",
        "# **5. Test Custom Model using Test Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV5CTwOAuSfT",
        "outputId": "5df2706b-d3ea-40f4-b50b-d0b6ccc77bfd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['yolov5l-cls.pt'], source=/content/sparkscon2024/scrape_images/apples/test/jonagored/pear-amidst-red-jonagored-apples-260nw-2389319923.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ðŸš€ v7.0-326-gec331cbd Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to yolov5l-cls.pt...\n",
            "100% 50.9M/50.9M [00:00<00:00, 295MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 215 layers, 26535976 parameters, 0 gradients, 69.3 GFLOPs\n",
            "image 1/1 /content/sparkscon2024/scrape_images/apples/test/jonagored/pear-amidst-red-jonagored-apples-260nw-2389319923.jpg: 224x224 Granny Smith 0.79, banana 0.05, grocery store 0.04, lemon 0.02, strawberry 0.01, 8.8ms\n",
            "Speed: 0.3ms pre-process, 8.8ms inference, 38.1ms NMS per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/predict-cls/exp4\u001b[0m\n",
            "1 labels saved to runs/predict-cls/exp4/labels\n"
          ]
        }
      ],
      "source": [
        "!python classify/predict.py --weights /content/sparkscon2024/models/weights/best.pt --source /content/sparkscon2024/scrape_images/apples/test/jonagored/pear-amidst-red-jonagored-apples-260nw-2389319923.jpg --save-txt\n",
        "\n",
        "### yolo_models_path:     /content/yolov5/runs/train-cls/yolov5l_results/weights/best.pt\n",
        "### standard yolo model:  yolov5l-cls.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stkwgjIXVXi7"
      },
      "source": [
        "---\n",
        "# **6. Export model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JLu_pmVeOYX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982588d0-ca5f-4c2a-e629-1fa5bddf8df7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/content/yolov5/runs/train-cls/yolov5l_results/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
            "YOLOv5 ðŸš€ v7.0-326-gec331cbd Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/export.py\", line 940, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/export.py\", line 935, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/export.py\", line 798, in run\n",
            "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
            "  File \"/content/yolov5/models/experimental.py\", line 98, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 997, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 444, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 425, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/yolov5/runs/train-cls/yolov5l_results/weights/best.pt'\n"
          ]
        }
      ],
      "source": [
        "!python export.py --weights /content/yolov5/runs/train-cls/yolov5l_results/weights/best.pt --include onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9DlkPgHd_Oi"
      },
      "source": [
        "---\n",
        "# **7. Load Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4knIuxLxZ3xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "71bd8d32-d853-4b1b-9194-57a6aa1328d0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.35 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model summary: 215 layers, 25258819 parameters, 0 gradients, 68.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ClassificationModel' object has no attribute 'yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-21fe12250c19>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Export the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mcustom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"imgsz\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"imgsz\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"verbose\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# method defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"export\"\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# highest priority args on the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     def train(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         )\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yaml_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"YOLO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Ultralytics {self.pretty_name} model {f\"trained on {data}\" if data else \"\"}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ClassificationModel' object has no attribute 'yaml'"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "# model = YOLO('yolov8n.pt')  # load an official model\n",
        "model = YOLO('/content/yolov5/runs/train-cls/yolov5l_results/weights/best.pt')  # load a custom trained model\n",
        "\n",
        "# Export the model\n",
        "model.export(format='onnx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDJX8G1vdqly"
      },
      "source": [
        "---\n",
        "# **Archive**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4EG87kj_wjw"
      },
      "outputs": [],
      "source": [
        "#Connect G-Drive to exchange data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265AiNodBsCW"
      },
      "outputs": [],
      "source": [
        "## required for Autobatch\n",
        "%cd ..\n",
        "!git clone https://github.com/ultralytics/ultralytics.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKQ5r-GdC-TV"
      },
      "outputs": [],
      "source": [
        "###!python /content/yolov5/ultralytics/ultralytics/utils/autobatch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxFiOaXyH3u8"
      },
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/scrape_images'"
      ],
      "metadata": {
        "id": "Q-OEsaV3eWym"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}